{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training for Cross-Modal Audience Intelligence\n",
    "\n",
    "This notebook demonstrates the end-to-end training process for the multimodal fusion model used in audience engagement prediction, including:\n",
    "- Data preparation\n",
    "- Feature extraction\n",
    "- Model architecture\n",
    "- Training process\n",
    "- Model evaluation\n",
    "- Model optimization\n",
    "- Saving trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Import platform components\n",
    "from models.fusion.fusion_model import MultimodalFusionModel\n",
    "from models.visual.clip_model import CLIPWrapper\n",
    "from models.text.roberta_model import RoBERTaWrapper\n",
    "from models.optimization.quantization import ModelQuantizer\n",
    "from models.optimization.onnx_export import ONNXExporter\n",
    "from data.data_loader import DataLoader as DataImporter\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation\n",
    "\n",
    "Let's load the preprocessed data and prepare it for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configure paths\n",
    "DATA_DIR = \"./data\"\n",
    "MODEL_DIR = \"./models/saved\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize data loader\n",
    "data_importer = DataImporter(cache_dir=f\"{DATA_DIR}/cache\")\n",
    "\n",
    "# Load processed data (assuming you've already run the data_exploration notebook)\n",
    "try:\n",
    "    # Try to load processed data from cache\n",
    "    processed_data = data_importer.load_processed_data()\n",
    "    print(f\"Loaded processed data from cache\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Processed data not found in cache. Creating sample data...\")\n",
    "    # Create sample data for demonstration\n",
    "    processed_data = {\n",
    "        \"content_ids\": [f\"SHOW{i}\" for i in range(100)],\n",
    "        \"text_content\": [f\"Sample description for content {i}\" for i in range(100)],\n",
    "        \"image_paths\": [f\"{DATA_DIR}/images/sample_{i}.jpg\" for i in range(100)],\n",
    "        \"engagement\": np.random.rand(100) * 0.8 + 0.1  # Random values between 0.1 and 0.9\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset for PyTorch\n",
    "\n",
    "We'll create a custom dataset for efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class AudienceDataset(Dataset):\n",
    "    \"\"\"Dataset for audience engagement prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, content_ids, text_content, image_paths, engagement, \n",
    "                 transform=None, text_transform=None):\n",
    "        \"\"\"Initialize dataset.\n",
    "        \n",
    "        Args:\n",
    "            content_ids: List of content identifiers\n",
    "            text_content: List of text descriptions\n",
    "            image_paths: List of paths to images\n",
    "            engagement: Array of engagement values\n",
    "            transform: Image transformation function\n",
    "            text_transform: Text transformation function\n",
    "        \"\"\"\n",
    "        self.content_ids = content_ids\n",
    "        self.text_content = text_content\n",
    "        self.image_paths = image_paths\n",
    "        self.engagement = engagement\n",
    "        self.transform = transform\n",
    "        self.text_transform = text_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.content_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get image\n",
    "        try:\n",
    "            # Try to load image\n",
    "            image_path = self.image_paths[idx]\n",
    "            if os.path.exists(image_path):\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "            else:\n",
    "                # Create a dummy image if file doesn't exist\n",
    "                image = torch.zeros((3, 224, 224))\n",
    "        except Exception as e:\n",
    "            # Create a dummy image if loading fails\n",
    "            print(f\"Error loading image {self.image_paths[idx]}: {e}\")\n",
    "            image = torch.zeros((3, 224, 224))\n",
    "        \n",
    "        # Get text\n",
    "        text = self.text_content[idx]\n",
    "        if self.text_transform:\n",
    "            text = self.text_transform(text)\n",
    "        \n",
    "        # Get engagement (target)\n",
    "        engagement = torch.tensor(self.engagement[idx], dtype=torch.float32)\n",
    "        \n",
    "        # Get content ID for reference\n",
    "        content_id = self.content_ids[idx]\n",
    "        \n",
    "        return {\n",
    "            'content_id': content_id,\n",
    "            'image': image,\n",
    "            'text': text,\n",
    "            'engagement': engagement\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize feature extractors for images and text\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize CLIP model for image features\n",
    "clip_model = CLIPWrapper(model_name=\"openai/clip-vit-base-patch32\", device=device)\n",
    "\n",
    "# Initialize RoBERTa model for text features\n",
    "roberta_model = RoBERTaWrapper(model_name=\"roberta-base\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define transformations\n",
    "def encode_image(image):\n",
    "    \"\"\"Encode image using CLIP.\"\"\"\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        # Handle dummy images\n",
    "        if image.shape == (3, 224, 224):\n",
    "            # Create a zero embedding\n",
    "            return torch.zeros((1, clip_model.model.config.projection_dim), device=device)\n",
    "    \n",
    "    # Encode real image\n",
    "    return clip_model.encode_images(image)\n",
    "\n",
    "def encode_text(text):\n",
    "    \"\"\"Encode text using RoBERTa.\"\"\"\n",
    "    return roberta_model.encode_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create dataset\n",
    "dataset = AudienceDataset(\n",
    "    content_ids=processed_data[\"content_ids\"],\n",
    "    text_content=processed_data[\"text_content\"],\n",
    "    image_paths=processed_data[\"image_paths\"],\n",
    "    engagement=processed_data[\"engagement\"],\n",
    "    transform=encode_image,\n",
    "    text_transform=encode_text\n",
    ")\n",
    "\n",
    "# Split into training and validation sets (80% training, 20% validation)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create data loaders\n",
    "batch_size = 16\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Fusion Model\n",
    "\n",
    "We'll create and initialize the multimodal fusion model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get feature dimensions\n",
    "visual_dim = clip_model.model.config.projection_dim  # Usually 768 for CLIP-ViT-B/32\n",
    "text_dim = roberta_model.model.config.hidden_size    # Usually 768 for RoBERTa-base\n",
    "\n",
    "print(f\"Visual feature dimension: {visual_dim}\")\n",
    "print(f\"Text feature dimension: {text_dim}\")\n",
    "\n",
    "# Initialize the fusion model\n",
    "fusion_model = MultimodalFusionModel(\n",
    "    visual_dim=visual_dim,\n",
    "    text_dim=text_dim,\n",
    "    fusion_dim=512,\n",
    "    num_layers=4,\n",
    "    num_heads=8,\n",
    "    feedforward_dim=2048,\n",
    "    dropout=0.1,\n",
    "    num_engagement_classes=5,  # For classification mode\n",
    "    engagement_type=\"regression\",  # Use regression for continuous engagement values\n",
    "    device=device\n",
    ")\n",
    "\n",
    "fusion_model.to(device)\n",
    "print(fusion_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Let's define the training loop and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(fusion_model.parameters(), lr=0.0001)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    \"\"\"Train model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        # Get data\n",
    "        visual_features = batch['image'].to(device)\n",
    "        text_features = batch['text'].to(device)\n",
    "        engagement = batch['engagement'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(visual_features, text_features)\n",
    "        \n",
    "        # Get predicted engagement\n",
    "        if model.engagement_type == \"regression\":\n",
    "            predicted_engagement = outputs[\"engagement\"][\"score\"].squeeze(-1)\n",
    "        else:  # classification\n",
    "            predicted_engagement = torch.argmax(outputs[\"engagement\"][\"probabilities\"], dim=1).float() / model.num_engagement_classes\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predicted_engagement, engagement)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Record loss\n",
    "        running_loss += loss.item() * visual_features.size(0)\n",
    "    \n",
    "    # Calculate epoch loss\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    \n",
    "    return epoch_loss\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate model.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validating\"):\n",
    "            # Get data\n",
    "            visual_features = batch['image'].to(device)\n",
    "            text_features = batch['text'].to(device)\n",
    "            engagement = batch['engagement'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(visual_features, text_features)\n",
    "            \n",
    "            # Get predicted engagement\n",
    "            if model.engagement_type == \"regression\":\n",
    "                predicted_engagement = outputs[\"engagement\"][\"score\"].squeeze(-1)\n",
    "            else:  # classification\n",
    "                predicted_engagement = torch.argmax(outputs[\"engagement\"][\"probabilities\"], dim=1).float() / model.num_engagement_classes\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(predicted_engagement, engagement)\n",
    "            \n",
    "            # Record loss\n",
    "            running_loss += loss.item() * visual_features.size(0)\n",
    "            \n",
    "            # Record predictions and targets for metrics\n",
    "            all_predictions.extend(predicted_engagement.cpu().numpy())\n",
    "            all_targets.extend(engagement.cpu().numpy())\n",
    "    \n",
    "    # Calculate validation loss\n",
    "    val_loss = running_loss / len(dataloader.dataset)\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    mse = mean_squared_error(all_targets, all_predictions)\n",
    "    r2 = r2_score(all_targets, all_predictions)\n",
    "    \n",
    "    return val_loss, mse, r2, all_predictions, all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 30\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Track metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_mses = []\n",
    "val_r2s = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(fusion_model, train_loader, optimizer, criterion, device)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_mse, val_r2, predictions, targets = validate(fusion_model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_mses.append(val_mse)\n",
    "    val_r2s.append(val_r2)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, MSE: {val_mse:.4f}, R²: {val_r2:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        print(f\"  Saving new best model with val_loss: {val_loss:.4f}\")\n",
    "        fusion_model.save(f\"{MODEL_DIR}/fusion_model_best.pt\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Progress\n",
    "\n",
    "Let's visualize the training and validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, num_epochs+1), train_losses, marker='o', linestyle='-', label='Training Loss')\n",
    "plt.plot(range(1, num_epochs+1), val_losses, marker='o', linestyle='-', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(f\"{MODEL_DIR}/training_loss.png\")\n",
    "plt.show()\n",
    "\n",
    "# Plot validation metrics\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# MSE on left axis\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('MSE', color='tab:red')\n",
    "ax1.plot(range(1, num_epochs+1), val_mses, marker='o', linestyle='-', color='tab:red', label='MSE')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "# R² on right axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('R²', color='tab:blue')\n",
    "ax2.plot(range(1, num_epochs+1), val_r2s, marker='o', linestyle='-', color='tab:blue', label='R²')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "# Add title and grid\n",
    "plt.title('Validation Metrics')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "\n",
    "plt.savefig(f\"{MODEL_DIR}/validation_metrics.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Let's evaluate the trained model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the best model\n",
    "best_model = MultimodalFusionModel.load(f\"{MODEL_DIR}/fusion_model_best.pt\", device=device)\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_loss, val_mse, val_r2, predictions, targets = validate(best_model, val_loader, criterion, device)\n",
    "\n",
    "print(f\"Final Validation Results:\")\n",
    "print(f\"  Loss: {val_loss:.4f}\")\n",
    "print(f\"  MSE: {val_mse:.4f}\")\n",
    "print(f\"  R²: {val_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize predictions vs targets\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(targets, predictions, alpha=0.5)\n",
    "plt.plot([min(targets), max(targets)], [min(targets), max(targets)], 'r--')  # Perfect prediction line\n",
    "plt.title('Predictions vs Targets')\n",
    "plt.xlabel('Actual Engagement')\n",
    "plt.ylabel('Predicted Engagement')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add metrics to plot\n",
    "plt.text(0.05, 0.95, f\"MSE: {val_mse:.4f}\\nR²: {val_r2:.4f}\", transform=plt.gca().transAxes,\n",
    "         bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.savefig(f\"{MODEL_DIR}/predictions_vs_targets.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimization\n",
    "\n",
    "Let's optimize the model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize quantizer\n",
    "quantizer = ModelQuantizer(best_model)\n",
    "\n",
    "# Quantize the model\n",
    "print(\"Quantizing model...\")\n",
    "quantized_model = quantizer.dynamic_quantization(dtype=\"int8\")\n",
    "\n",
    "# Save quantized model\n",
    "torch.save(quantized_model, f\"{MODEL_DIR}/fusion_model_quantized.pt\")\n",
    "print(f\"Saved quantized model to {MODEL_DIR}/fusion_model_quantized.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Benchmark original vs quantized model\n",
    "# Create sample inputs for benchmarking\n",
    "sample_visual = torch.randn(1, visual_dim).to(device)\n",
    "sample_text = torch.randn(1, text_dim).to(device)\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_results = quantizer.benchmark(\n",
    "    input_data=(sample_visual, sample_text),\n",
    "    num_runs=100,\n",
    "    warmup_runs=10\n",
    ")\n",
    "\n",
    "# Print benchmark results\n",
    "print(\"Benchmark Results:\")\n",
    "print(f\"  Original model inference time: {benchmark_results['original_time_s']*1000:.2f} ms\")\n",
    "print(f\"  Quantized model inference time: {benchmark_results['quantized_time_s']*1000:.2f} ms\")\n",
    "print(f\"  Speedup: {benchmark_results['speedup_factor']:.2f}x\")\n",
    "print(f\"  Original model size: {benchmark_results['original_size_mb']:.2f} MB\")\n",
    "print(f\"  Quantized model size: {benchmark_results['quantized_size_mb']:.2f} MB\")\n",
    "print(f\"  Size reduction: {benchmark_results['size_reduction']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Export to ONNX format\n",
    "# Initialize ONNX exporter\n",
    "onnx_exporter = ONNXExporter(best_model, device=device)\n",
    "\n",
    "# Export model to ONNX\n",
    "print(\"Exporting model to ONNX...\")\n",
    "onnx_path = onnx_exporter.export(\n",
    "    dummy_input=(sample_visual, sample_text),\n",
    "    output_path=f\"{MODEL_DIR}/fusion_model.onnx\",\n",
    "    input_names=[\"visual_features\", \"text_features\"],\n",
    "    output_names=[\"engagement\", \"sentiment\", \"content_features\"],\n",
    "    verbose=True,\n",
    "    optimize=True\n",
    ")\n",
    "\n",
    "print(f\"Exported ONNX model to {onnx_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model Configuration\n",
    "\n",
    "Let's save the model configuration for easy loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save model configuration\n",
    "model_config = {\n",
    "    \"model_type\": \"MultimodalFusionModel\",\n",
    "    \"visual_dim\": visual_dim,\n",
    "    \"text_dim\": text_dim,\n",
    "    \"fusion_dim\": 512,\n",
    "    \"num_layers\": 4,\n",
    "    \"num_heads\": 8,\n",
    "    \"engagement_type\": \"regression\",\n",
    "    \"training_info\": {\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "        \"final_mse\": val_mse,\n",
    "        \"final_r2\": val_r2,\n",
    "        \"date_trained\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    },\n",
    "    \"files\": {\n",
    "        \"pytorch_model\": \"fusion_model_best.pt\",\n",
    "        \"quantized_model\": \"fusion_model_quantized.pt\",\n",
    "        \"onnx_model\": \"fusion_model.onnx\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "with open(f\"{MODEL_DIR}/model_config.json\", \"w\") as f:\n",
    "    json.dump(model_config, f, indent=2)\n",
    "\n",
    "print(f\"Saved model configuration to {MODEL_DIR}/model_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we demonstrated the end-to-end training process for the multimodal fusion model used in audience engagement prediction:\n",
    "\n",
    "1. We prepared the data and created a custom PyTorch dataset\n",
    "2. We initialized the fusion model architecture\n",
    "3. We trained the model and tracked its performance\n",
    "4. We evaluated the model on validation data\n",
    "5. We optimized the model through quantization and ONNX export\n",
    "6. We saved the model configuration for future use\n",
    "\n",
    "The trained model can now be used for audience engagement prediction in production systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}