{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training for Cross-Modal Audience Intelligence\n",
    "\n",
    "This notebook demonstrates the end-to-end training process for the multimodal fusion model used in audience engagement prediction, including:\n",
    "- Data preparation\n",
    "- Feature extraction\n",
    "- Model architecture\n",
    "- Training process\n",
    "- Model evaluation\n",
    "- Model optimization\n",
    "- Saving trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (1.25.2)\n",
      "Requirement already satisfied: matplotlib in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (3.8.0)\n",
      "Requirement already satisfied: seaborn in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (0.13.0)\n",
      "Requirement already satisfied: networkx in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (3.4.2)\n",
      "Requirement already satisfied: scipy in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (1.15.1)\n",
      "Requirement already satisfied: tqdm in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: torch in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: torchvision in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (0.16.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: pillow in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (10.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: filelock in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torch) (3.15.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: jinja2 in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: requests in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from requests->torchvision) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from requests->torchvision) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from requests->torchvision) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from requests->torchvision) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement caip (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for caip\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting onnx\n",
      "  Downloading onnx-1.17.0-cp311-cp311-macosx_12_0_universal2.whl.metadata (16 kB)\n",
      "Requirement already satisfied: onnxruntime in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (1.21.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from onnx) (1.25.2)\n",
      "Collecting protobuf>=3.20.2 (from onnx)\n",
      "  Using cached protobuf-6.30.2-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: coloredlogs in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from onnxruntime) (25.1.24)\n",
      "Requirement already satisfied: packaging in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from onnxruntime) (23.2)\n",
      "Requirement already satisfied: sympy in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from onnxruntime) (1.13.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/gopalmacbook/.pyenv/versions/3.11.6/lib/python3.11/site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Downloading onnx-1.17.0-cp311-cp311-macosx_12_0_universal2.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.30.2-cp39-abi3-macosx_10_9_universal2.whl (417 kB)\n",
      "Installing collected packages: protobuf, onnx\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.0\n",
      "    Uninstalling protobuf-3.20.0:\n",
      "      Successfully uninstalled protobuf-3.20.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.25.2 which is incompatible.\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.30.2 which is incompatible.\n",
      "streamlit 1.29.0 requires protobuf<5,>=3.20, but you have protobuf 6.30.2 which is incompatible.\n",
      "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.30.2 which is incompatible.\n",
      "grpcio-status 1.70.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.30.2 which is incompatible.\n",
      "googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 6.30.2 which is incompatible.\n",
      "google-api-core 2.23.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5, but you have protobuf 6.30.2 which is incompatible.\n",
      "proto-plus 1.25.0 requires protobuf<6.0.0dev,>=3.19.0, but you have protobuf 6.30.2 which is incompatible.\n",
      "opentelemetry-proto 1.27.0 requires protobuf<5.0,>=3.19, but you have protobuf 6.30.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed onnx-1.17.0 protobuf-6.30.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy matplotlib seaborn networkx scipy tqdm torch torchvision scikit-learn pillow\n",
    "%pip install caip \n",
    "%pip install onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Import platform components\n",
    "from models.fusion.fusion_model import MultimodalFusionModel\n",
    "from models.visual.clip_model import CLIPWrapper\n",
    "from models.text.roberta_model import RoBERTaWrapper\n",
    "from models.optimization.quantization import ModelQuantizer\n",
    "from models.optimization.onnx_export import ONNXExporter\n",
    "from data.data_loader import DataLoader as DataImporter\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation\n",
    "\n",
    "Let's load the preprocessed data and prepare it for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths\n",
    "DATA_DIR = \"./data\"\n",
    "MODEL_DIR = \"./models/saved\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize data loader\n",
    "data_importer = DataImporter(cache_dir=f\"{DATA_DIR}/cache\")\n",
    "\n",
    "# Load processed data (assuming you've already run the data_exploration notebook)\n",
    "try:\n",
    "    # Try to load processed data from cache\n",
    "    processed_data = data_importer.load_processed_data()\n",
    "    print(f\"Loaded processed data from cache\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Processed data not found in cache. Creating sample data...\")\n",
    "    # Create sample data for demonstration\n",
    "    processed_data = {\n",
    "        \"content_ids\": [f\"SHOW{i}\" for i in range(100)],\n",
    "        \"text_content\": [f\"Sample description for content {i}\" for i in range(100)],\n",
    "        \"image_paths\": [f\"{DATA_DIR}/images/sample_{i}.jpg\" for i in range(100)],\n",
    "        \"engagement\": np.random.rand(100) * 0.8 + 0.1  # Random values between 0.1 and 0.9\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset for PyTorch\n",
    "\n",
    "We'll create a custom dataset for efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudienceDataset(Dataset):\n",
    "    \"\"\"Dataset for audience engagement prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, content_ids, text_content, image_paths, engagement, \n",
    "                 transform=None, text_transform=None):\n",
    "        \"\"\"Initialize dataset.\n",
    "        \n",
    "        Args:\n",
    "            content_ids: List of content identifiers\n",
    "            text_content: List of text descriptions\n",
    "            image_paths: List of paths to images\n",
    "            engagement: Array of engagement values\n",
    "            transform: Image transformation function\n",
    "            text_transform: Text transformation function\n",
    "        \"\"\"\n",
    "        self.content_ids = content_ids\n",
    "        self.text_content = text_content\n",
    "        self.image_paths = image_paths\n",
    "        self.engagement = engagement\n",
    "        self.transform = transform\n",
    "        self.text_transform = text_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.content_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get image\n",
    "        try:\n",
    "            # Try to load image\n",
    "            image_path = self.image_paths[idx]\n",
    "            if os.path.exists(image_path):\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "            else:\n",
    "                # Create a dummy image if file doesn't exist\n",
    "                image = torch.zeros((3, 224, 224))\n",
    "        except Exception as e:\n",
    "            # Create a dummy image if loading fails\n",
    "            print(f\"Error loading image {self.image_paths[idx]}: {e}\")\n",
    "            image = torch.zeros((3, 224, 224))\n",
    "        \n",
    "        # Get text\n",
    "        text = self.text_content[idx]\n",
    "        if self.text_transform:\n",
    "            text = self.text_transform(text)\n",
    "        \n",
    "        # Get engagement (target)\n",
    "        engagement = torch.tensor(self.engagement[idx], dtype=torch.float32)\n",
    "        \n",
    "        # Get content ID for reference\n",
    "        content_id = self.content_ids[idx]\n",
    "        \n",
    "        return {\n",
    "            'content_id': content_id,\n",
    "            'image': image,\n",
    "            'text': text,\n",
    "            'engagement': engagement\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature extractors for images and text\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize CLIP model for image features\n",
    "clip_model = CLIPWrapper(model_name=\"openai/clip-vit-base-patch32\", device=device)\n",
    "\n",
    "# Initialize RoBERTa model for text features\n",
    "roberta_model = RoBERTaWrapper(model_name=\"roberta-base\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "def encode_image(image):\n",
    "    \"\"\"Encode image using CLIP.\"\"\"\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        # Handle dummy images\n",
    "        if image.shape == (3, 224, 224):\n",
    "            # Create a zero embedding\n",
    "            return torch.zeros((1, clip_model.model.config.projection_dim), device=device)\n",
    "    \n",
    "    # Encode real image\n",
    "    return clip_model.encode_images(image)\n",
    "\n",
    "def encode_text(text):\n",
    "    \"\"\"Encode text using RoBERTa.\"\"\"\n",
    "    return roberta_model.encode_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "dataset = AudienceDataset(\n",
    "    content_ids=processed_data[\"content_ids\"],\n",
    "    text_content=processed_data[\"text_content\"],\n",
    "    image_paths=processed_data[\"image_paths\"],\n",
    "    engagement=processed_data[\"engagement\"],\n",
    "    transform=encode_image,\n",
    "    text_transform=encode_text\n",
    ")\n",
    "\n",
    "# Split into training and validation sets (80% training, 20% validation)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 16\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Fusion Model\n",
    "\n",
    "We'll create and initialize the multimodal fusion model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature dimensions\n",
    "visual_dim = clip_model.model.config.projection_dim  # Usually 768 for CLIP-ViT-B/32\n",
    "text_dim = roberta_model.model.config.hidden_size    # Usually 768 for RoBERTa-base\n",
    "\n",
    "print(f\"Visual feature dimension: {visual_dim}\")\n",
    "print(f\"Text feature dimension: {text_dim}\")\n",
    "\n",
    "# Initialize the fusion model\n",
    "fusion_model = MultimodalFusionModel(\n",
    "    visual_dim=visual_dim,\n",
    "    text_dim=text_dim,\n",
    "    fusion_dim=512,\n",
    "    num_layers=4,\n",
    "    num_heads=8,\n",
    "    feedforward_dim=2048,\n",
    "    dropout=0.1,\n",
    "    num_engagement_classes=5,  # For classification mode\n",
    "    engagement_type=\"regression\",  # Use regression for continuous engagement values\n",
    "    device=device\n",
    ")\n",
    "\n",
    "fusion_model.to(device)\n",
    "print(fusion_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Let's define the training loop and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(fusion_model.parameters(), lr=0.0001)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    \"\"\"Train model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        # Get data\n",
    "        visual_features = batch['image'].to(device)\n",
    "        text_features = batch['text'].to(device)\n",
    "        engagement = batch['engagement'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(visual_features, text_features)\n",
    "        \n",
    "        # Get predicted engagement\n",
    "        if model.engagement_type == \"regression\":\n",
    "            predicted_engagement = outputs[\"engagement\"][\"score\"].squeeze(-1)\n",
    "        else:  # classification\n",
    "            predicted_engagement = torch.argmax(outputs[\"engagement\"][\"probabilities\"], dim=1).float() / model.num_engagement_classes\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predicted_engagement, engagement)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Record loss\n",
    "        running_loss += loss.item() * visual_features.size(0)\n",
    "    \n",
    "    # Calculate epoch loss\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    \n",
    "    return epoch_loss\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate model.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validating\"):\n",
    "            # Get data\n",
    "            visual_features = batch['image'].to(device)\n",
    "            text_features = batch['text'].to(device)\n",
    "            engagement = batch['engagement'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(visual_features, text_features)\n",
    "            \n",
    "            # Get predicted engagement\n",
    "            if model.engagement_type == \"regression\":\n",
    "                predicted_engagement = outputs[\"engagement\"][\"score\"].squeeze(-1)\n",
    "            else:  # classification\n",
    "                predicted_engagement = torch.argmax(outputs[\"engagement\"][\"probabilities\"], dim=1).float() / model.num_engagement_classes\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(predicted_engagement, engagement)\n",
    "            \n",
    "            # Record loss\n",
    "            running_loss += loss.item() * visual_features.size(0)\n",
    "            \n",
    "            # Record predictions and targets for metrics\n",
    "            all_predictions.extend(predicted_engagement.cpu().numpy())\n",
    "            all_targets.extend(engagement.cpu().numpy())\n",
    "    \n",
    "    # Calculate validation loss\n",
    "    val_loss = running_loss / len(dataloader.dataset)\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    mse = mean_squared_error(all_targets, all_predictions)\n",
    "    r2 = r2_score(all_targets, all_predictions)\n",
    "    \n",
    "    return val_loss, mse, r2, all_predictions, all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 30\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Track metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_mses = []\n",
    "val_r2s = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(fusion_model, train_loader, optimizer, criterion, device)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_mse, val_r2, predictions, targets = validate(fusion_model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_mses.append(val_mse)\n",
    "    val_r2s.append(val_r2)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, MSE: {val_mse:.4f}, R²: {val_r2:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        print(f\"  Saving new best model with val_loss: {val_loss:.4f}\")\n",
    "        fusion_model.save(f\"{MODEL_DIR}/fusion_model_best.pt\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Progress\n",
    "\n",
    "Let's visualize the training and validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, num_epochs+1), train_losses, marker='o', linestyle='-', label='Training Loss')\n",
    "plt.plot(range(1, num_epochs+1), val_losses, marker='o', linestyle='-', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(f\"{MODEL_DIR}/training_loss.png\")\n",
    "plt.show()\n",
    "\n",
    "# Plot validation metrics\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# MSE on left axis\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('MSE', color='tab:red')\n",
    "ax1.plot(range(1, num_epochs+1), val_mses, marker='o', linestyle='-', color='tab:red', label='MSE')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "# R² on right axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('R²', color='tab:blue')\n",
    "ax2.plot(range(1, num_epochs+1), val_r2s, marker='o', linestyle='-', color='tab:blue', label='R²')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "# Add title and grid\n",
    "plt.title('Validation Metrics')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "\n",
    "plt.savefig(f\"{MODEL_DIR}/validation_metrics.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Let's evaluate the trained model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_model = MultimodalFusionModel.load(f\"{MODEL_DIR}/fusion_model_best.pt\", device=device)\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_loss, val_mse, val_r2, predictions, targets = validate(best_model, val_loader, criterion, device)\n",
    "\n",
    "print(f\"Final Validation Results:\")\n",
    "print(f\"  Loss: {val_loss:.4f}\")\n",
    "print(f\"  MSE: {val_mse:.4f}\")\n",
    "print(f\"  R²: {val_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs targets\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(targets, predictions, alpha=0.5)\n",
    "plt.plot([min(targets), max(targets)], [min(targets), max(targets)], 'r--')  # Perfect prediction line\n",
    "plt.title('Predictions vs Targets')\n",
    "plt.xlabel('Actual Engagement')\n",
    "plt.ylabel('Predicted Engagement')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add metrics to plot\n",
    "plt.text(0.05, 0.95, f\"MSE: {val_mse:.4f}\\nR²: {val_r2:.4f}\", transform=plt.gca().transAxes,\n",
    "         bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.savefig(f\"{MODEL_DIR}/predictions_vs_targets.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimization\n",
    "\n",
    "Let's optimize the model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize quantizer\n",
    "quantizer = ModelQuantizer(best_model)\n",
    "\n",
    "# Quantize the model\n",
    "print(\"Quantizing model...\")\n",
    "quantized_model = quantizer.dynamic_quantization(dtype=\"int8\")\n",
    "\n",
    "# Save quantized model\n",
    "torch.save(quantized_model, f\"{MODEL_DIR}/fusion_model_quantized.pt\")\n",
    "print(f\"Saved quantized model to {MODEL_DIR}/fusion_model_quantized.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark original vs quantized model\n",
    "# Create sample inputs for benchmarking\n",
    "sample_visual = torch.randn(1, visual_dim).to(device)\n",
    "sample_text = torch.randn(1, text_dim).to(device)\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_results = quantizer.benchmark(\n",
    "    input_data=(sample_visual, sample_text),\n",
    "    num_runs=100,\n",
    "    warmup_runs=10\n",
    ")\n",
    "\n",
    "# Print benchmark results\n",
    "print(\"Benchmark Results:\")\n",
    "print(f\"  Original model inference time: {benchmark_results['original_time_s']*1000:.2f} ms\")\n",
    "print(f\"  Quantized model inference time: {benchmark_results['quantized_time_s']*1000:.2f} ms\")\n",
    "print(f\"  Speedup: {benchmark_results['speedup_factor']:.2f}x\")\n",
    "print(f\"  Original model size: {benchmark_results['original_size_mb']:.2f} MB\")\n",
    "print(f\"  Quantized model size: {benchmark_results['quantized_size_mb']:.2f} MB\")\n",
    "print(f\"  Size reduction: {benchmark_results['size_reduction']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX format\n",
    "# Initialize ONNX exporter\n",
    "onnx_exporter = ONNXExporter(best_model, device=device)\n",
    "\n",
    "# Export model to ONNX\n",
    "print(\"Exporting model to ONNX...\")\n",
    "onnx_path = onnx_exporter.export(\n",
    "    dummy_input=(sample_visual, sample_text),\n",
    "    output_path=f\"{MODEL_DIR}/fusion_model.onnx\",\n",
    "    input_names=[\"visual_features\", \"text_features\"],\n",
    "    output_names=[\"engagement\", \"sentiment\", \"content_features\"],\n",
    "    verbose=True,\n",
    "    optimize=True\n",
    ")\n",
    "\n",
    "print(f\"Exported ONNX model to {onnx_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model Configuration\n",
    "\n",
    "Let's save the model configuration for easy loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model configuration\n",
    "model_config = {\n",
    "    \"model_type\": \"MultimodalFusionModel\",\n",
    "    \"visual_dim\": visual_dim,\n",
    "    \"text_dim\": text_dim,\n",
    "    \"fusion_dim\": 512,\n",
    "    \"num_layers\": 4,\n",
    "    \"num_heads\": 8,\n",
    "    \"engagement_type\": \"regression\",\n",
    "    \"training_info\": {\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "        \"final_mse\": val_mse,\n",
    "        \"final_r2\": val_r2,\n",
    "        \"date_trained\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    },\n",
    "    \"files\": {\n",
    "        \"pytorch_model\": \"fusion_model_best.pt\",\n",
    "        \"quantized_model\": \"fusion_model_quantized.pt\",\n",
    "        \"onnx_model\": \"fusion_model.onnx\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "with open(f\"{MODEL_DIR}/model_config.json\", \"w\") as f:\n",
    "    json.dump(model_config, f, indent=2)\n",
    "\n",
    "print(f\"Saved model configuration to {MODEL_DIR}/model_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we demonstrated the end-to-end training process for the multimodal fusion model used in audience engagement prediction:\n",
    "\n",
    "1. We prepared the data and created a custom PyTorch dataset\n",
    "2. We initialized the fusion model architecture\n",
    "3. We trained the model and tracked its performance\n",
    "4. We evaluated the model on validation data\n",
    "5. We optimized the model through quantization and ONNX export\n",
    "6. We saved the model configuration for future use\n",
    "\n",
    "The trained model can now be used for audience engagement prediction in production systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
