{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration for Cross-Modal Audience Intelligence\n",
    "\n",
    "This notebook explores the multimodal dataset used for audience engagement prediction, including:\n",
    "- Data loading and inspection\n",
    "- Exploratory data analysis\n",
    "- Visual content analysis\n",
    "- Text content analysis\n",
    "- Feature correlations and distributions\n",
    "- Engagement metric analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# Import platform components\n",
    "from data.data_loader import DataLoader\n",
    "from data.connectors.nielsen_connector import NielsenConnector\n",
    "from data.connectors.streaming_api import StreamingPlatformConnector\n",
    "from data.connectors.social_crawler import SocialMediaCrawler\n",
    "from data.preprocessing.text_preprocessor import TextPreprocessor\n",
    "from data.preprocessing.image_preprocessor import ImagePreprocessor\n",
    "from data.preprocessing.feature_engineering import FeatureEngineer\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "We'll load data from multiple sources, including Nielsen panel data, streaming platform data, and social media data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize data connectors\n",
    "nielsen_connector = NielsenConnector(api_key=\"YOUR_API_KEY\", cache_dir=\"./data/cache\")\n",
    "netflix_connector = StreamingPlatformConnector(\"netflix\", api_key=\"YOUR_API_KEY\", cache_dir=\"./data/cache\")\n",
    "social_crawler = SocialMediaCrawler(platforms=[\"twitter\", \"instagram\"], cache_dir=\"./data/cache\")\n",
    "\n",
    "# Initialize data loader\n",
    "data_loader = DataLoader(\n",
    "    nielsen_connector=nielsen_connector,\n",
    "    streaming_connectors={\"netflix\": netflix_connector},\n",
    "    social_crawler=social_crawler,\n",
    "    cache_dir=\"./data/cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load audience data for sample content\n",
    "audience_data = data_loader.load_audience_data(\n",
    "    content_ids=[\"SHOW123\", \"SHOW456\", \"SHOW789\"],\n",
    "    content_names=[\"Sample Show 1\", \"Sample Show 2\", \"Sample Show 3\"],\n",
    "    start_date=pd.Timestamp(\"2023-01-01\").date(),\n",
    "    end_date=pd.Timestamp(\"2023-01-31\").date(),\n",
    "    metrics=[\"views\", \"engagement\", \"completion_rate\"],\n",
    "    include_social=True,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "# Display data sources\n",
    "print(f\"Data sources: {list(audience_data.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display Nielsen panel data\n",
    "nielsen_df = audience_data.get(\"nielsen\")\n",
    "print(f\"Nielsen data shape: {nielsen_df.shape}\")\n",
    "nielsen_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display streaming platform data\n",
    "streaming_df = audience_data.get(\"streaming\")\n",
    "print(f\"Streaming data shape: {streaming_df.shape}\")\n",
    "streaming_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display social media data\n",
    "social_df = audience_data.get(\"social\")\n",
    "print(f\"Social media data shape: {social_df.shape}\")\n",
    "social_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Summary Statistics\n",
    "\n",
    "Let's examine the basic statistics of our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Nielsen data summary\n",
    "print(\"Nielsen data summary:\")\n",
    "nielsen_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Streaming data summary\n",
    "print(\"Streaming data summary:\")\n",
    "streaming_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate engagement statistics by content\n",
    "engagement_by_content = streaming_df.groupby('content_id').agg({\n",
    "    'views': ['mean', 'median', 'min', 'max', 'std'],\n",
    "    'engagement': ['mean', 'median', 'min', 'max', 'std'],\n",
    "    'completion_rate': ['mean', 'median', 'min', 'max', 'std']\n",
    "})\n",
    "\n",
    "engagement_by_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "Let's visualize the data to better understand distributions and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize engagement metrics across content\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.boxplot(x='content_id', y='views', data=streaming_df)\n",
    "plt.title('Views by Content')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.boxplot(x='content_id', y='engagement', data=streaming_df)\n",
    "plt.title('Engagement by Content')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.boxplot(x='content_id', y='completion_rate', data=streaming_df)\n",
    "plt.title('Completion Rate by Content')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize correlation between metrics\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = streaming_df[['views', 'engagement', 'completion_rate']].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt='.2f')\n",
    "plt.title('Correlation Matrix of Engagement Metrics')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize social media sentiment over time\n",
    "if 'post_date' in social_df.columns and 'sentiment' in social_df.columns:\n",
    "    social_df['post_date'] = pd.to_datetime(social_df['post_date'])\n",
    "    social_df.set_index('post_date', inplace=True)\n",
    "    \n",
    "    # Group by day and platform\n",
    "    daily_sentiment = social_df.groupby([pd.Grouper(freq='D'), 'platform'])['sentiment'].mean().reset_index()\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.lineplot(x='post_date', y='sentiment', hue='platform', data=daily_sentiment, marker='o')\n",
    "    plt.title('Average Sentiment by Platform Over Time')\n",
    "    plt.ylabel('Sentiment Score')\n",
    "    plt.xlabel('Date')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Content Analysis\n",
    "\n",
    "Let's load and analyze some sample images from our content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to load sample images\n",
    "def load_sample_images(content_ids, image_dir=\"./data/images\"):\n",
    "    images = {}\n",
    "    for content_id in content_ids:\n",
    "        image_path = Path(image_dir) / f\"{content_id}.jpg\"\n",
    "        if image_path.exists():\n",
    "            images[content_id] = Image.open(image_path)\n",
    "    return images\n",
    "\n",
    "# Load sample images\n",
    "sample_images = load_sample_images([\"SHOW123\", \"SHOW456\", \"SHOW789\"])\n",
    "\n",
    "# Display sample images\n",
    "if sample_images:\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    for i, (content_id, img) in enumerate(sample_images.items()):\n",
    "        plt.subplot(1, len(sample_images), i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Content ID: {content_id}\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No sample images found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize image preprocessor\n",
    "image_preprocessor = ImagePreprocessor(target_size=(224, 224), normalize=True)\n",
    "\n",
    "# Preprocess sample images\n",
    "if sample_images:\n",
    "    processed_images = {}\n",
    "    for content_id, img in sample_images.items():\n",
    "        processed_images[content_id] = image_preprocessor.preprocess(img)\n",
    "    \n",
    "    print(f\"Processed image tensor shape: {next(iter(processed_images.values())).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Content Analysis\n",
    "\n",
    "Let's analyze the text content from descriptions, social media posts, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract text content from social media data\n",
    "if 'text' in social_df.columns:\n",
    "    text_content = social_df['text'].dropna().tolist()\n",
    "    print(f\"Number of text samples: {len(text_content)}\")\n",
    "    \n",
    "    # Display some sample texts\n",
    "    for i, text in enumerate(text_content[:5]):\n",
    "        print(f\"Sample {i+1}: {text[:100]}...\" if len(text) > 100 else f\"Sample {i+1}: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize text preprocessor\n",
    "text_preprocessor = TextPreprocessor(remove_stopwords=True, lowercase=True)\n",
    "\n",
    "# Preprocess sample texts\n",
    "if 'text' in social_df.columns:\n",
    "    processed_texts = []\n",
    "    for text in text_content[:5]:\n",
    "        clean_text = text_preprocessor.clean_text(text)\n",
    "        tokens = text_preprocessor.tokenize(clean_text)\n",
    "        processed_texts.append({\n",
    "            'original': text,\n",
    "            'cleaned': clean_text,\n",
    "            'tokens': tokens\n",
    "        })\n",
    "    \n",
    "    # Display processed texts\n",
    "    for i, item in enumerate(processed_texts):\n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(f\"Original: {item['original'][:50]}...\")\n",
    "        print(f\"Cleaned: {item['cleaned'][:50]}...\")\n",
    "        print(f\"Tokens: {', '.join(item['tokens'][:10])}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze word frequency in text content\n",
    "if 'text' in social_df.columns:\n",
    "    from collections import Counter\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    # Download NLTK resources if needed\n",
    "    try:\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "    except LookupError:\n",
    "        nltk.download('stopwords')\n",
    "    \n",
    "    # Process all texts and count word frequency\n",
    "    all_words = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    for text in text_content:\n",
    "        clean_text = text_preprocessor.clean_text(text)\n",
    "        tokens = text_preprocessor.tokenize(clean_text)\n",
    "        all_words.extend([token for token in tokens if token not in stop_words])\n",
    "    \n",
    "    word_counts = Counter(all_words)\n",
    "    top_words = word_counts.most_common(20)\n",
    "    \n",
    "    # Plot word frequency\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    words, counts = zip(*top_words)\n",
    "    plt.bar(words, counts)\n",
    "    plt.title('Top 20 Words in Social Media Content')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.xlabel('Word')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Let's perform feature engineering to prepare data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize feature engineer\n",
    "feature_engineer = FeatureEngineer(scaler=\"standard\")\n",
    "\n",
    "# Prepare features from streaming data\n",
    "if not streaming_df.empty:\n",
    "    # Select relevant columns for features\n",
    "    feature_cols = ['views', 'completion_rate']\n",
    "    if 'average_watch_time' in streaming_df.columns:\n",
    "        feature_cols.append('average_watch_time')\n",
    "    \n",
    "    # Add temporal features if date column exists\n",
    "    if 'date' in streaming_df.columns:\n",
    "        streaming_df = feature_engineer.add_temporal_features(streaming_df, date_columns=['date'])\n",
    "        # Add temporal features to our feature columns\n",
    "        feature_cols.extend(['date_month', 'date_day', 'date_dayofweek'])\n",
    "    \n",
    "    # Fit the feature engineer\n",
    "    feature_engineer.fit(streaming_df[feature_cols], target_col='engagement')\n",
    "    \n",
    "    # Transform the features\n",
    "    feature_matrix = feature_engineer.transform(streaming_df[feature_cols])\n",
    "    \n",
    "    print(f\"Feature matrix shape: {feature_matrix.shape}\")\n",
    "    \n",
    "    # Create a DataFrame with the transformed features\n",
    "    feature_df = pd.DataFrame(\n",
    "        feature_matrix,\n",
    "        columns=feature_engineer.get_feature_names(feature_cols)\n",
    "    )\n",
    "    \n",
    "    # Display the engineered features\n",
    "    feature_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engagement Analysis\n",
    "\n",
    "Let's analyze patterns in audience engagement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate engagement metrics over time\n",
    "if 'date' in streaming_df.columns and 'engagement' in streaming_df.columns:\n",
    "    streaming_df['date'] = pd.to_datetime(streaming_df['date'])\n",
    "    \n",
    "    # Group by date and content_id\n",
    "    daily_engagement = streaming_df.groupby([pd.Grouper(key='date', freq='D'), 'content_id'])['engagement'].mean().unstack()\n",
    "    \n",
    "    # Plot engagement over time\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    daily_engagement.plot(marker='o', ax=plt.gca())\n",
    "    plt.title('Daily Engagement by Content')\n",
    "    plt.ylabel('Engagement Score')\n",
    "    plt.xlabel('Date')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(title='Content ID')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze correlation between social sentiment and engagement\n",
    "if 'sentiment' in social_df.columns and 'engagement' in streaming_df.columns:\n",
    "    # Convert post_date to datetime if needed\n",
    "    if 'post_date' in social_df.columns and not pd.api.types.is_datetime64_any_dtype(social_df['post_date']):\n",
    "        social_df['post_date'] = pd.to_datetime(social_df['post_date'])\n",
    "    \n",
    "    # Aggregate sentiment by content and date\n",
    "    if 'content_name' in social_df.columns and 'post_date' in social_df.columns:\n",
    "        daily_sentiment = social_df.groupby(['content_name', pd.Grouper(key='post_date', freq='D')])['sentiment'].mean().reset_index()\n",
    "        \n",
    "        # Map content_name to content_id if needed\n",
    "        content_mapping = {\n",
    "            'Sample Show 1': 'SHOW123',\n",
    "            'Sample Show 2': 'SHOW456',\n",
    "            'Sample Show 3': 'SHOW789'\n",
    "        }\n",
    "        daily_sentiment['content_id'] = daily_sentiment['content_name'].map(content_mapping)\n",
    "        \n",
    "        # Merge with streaming data\n",
    "        if 'date' in streaming_df.columns:\n",
    "            # Ensure date is datetime\n",
    "            if not pd.api.types.is_datetime64_any_dtype(streaming_df['date']):\n",
    "                streaming_df['date'] = pd.to_datetime(streaming_df['date'])\n",
    "                \n",
    "            # Aggregate engagement by content and date\n",
    "            daily_engagement = streaming_df.groupby(['content_id', pd.Grouper(key='date', freq='D')])['engagement'].mean().reset_index()\n",
    "            \n",
    "            # Merge datasets\n",
    "            sentiment_engagement = pd.merge(\n",
    "                daily_sentiment,\n",
    "                daily_engagement,\n",
    "                left_on=['content_id', 'post_date'],\n",
    "                right_on=['content_id', 'date'],\n",
    "                how='inner'\n",
    "            )\n",
    "            \n",
    "            # Plot correlation\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.scatterplot(x='sentiment', y='engagement', hue='content_id', data=sentiment_engagement)\n",
    "            plt.title('Correlation between Social Media Sentiment and Engagement')\n",
    "            plt.xlabel('Sentiment Score')\n",
    "            plt.ylabel('Engagement Score')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add correlation line\n",
    "            from scipy import stats\n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(sentiment_engagement['sentiment'], sentiment_engagement['engagement'])\n",
    "            plt.plot(sentiment_engagement['sentiment'], intercept + slope*sentiment_engagement['sentiment'], 'r', alpha=0.7)\n",
    "            plt.annotate(f'r = {r_value:.2f}, p = {p_value:.4f}', xy=(0.05, 0.95), xycoords='axes fraction')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we explored multimodal data related to audience engagement, including:\n",
    "- Basic data loading and inspection\n",
    "- Distribution and correlation of engagement metrics\n",
    "- Analysis of visual content\n",
    "- Analysis of text content\n",
    "- Feature engineering for model training\n",
    "- Relationship between social sentiment and engagement\n",
    "\n",
    "These insights will be used to inform model training and causal analysis in subsequent notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}